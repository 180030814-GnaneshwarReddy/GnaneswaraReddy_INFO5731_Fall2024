{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/180030814-GnaneshwarReddy/GnaneswaraReddy_INFO5731_Fall2024/blob/main/Palem_Gnaneswara_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "My_Scrapper_Api_Key = '3b1bf40341e8e4da451c5ac266749f96'\n",
        "\n",
        "amazon_product_url = 'https://www.amazon.com/Apple-iPhone-13-128GB-Midnight/dp/B09LNW3CY2'  # Change to any product you like\n",
        "\n",
        "def scrape_amazon_reviews(url, api_key):\n",
        "    api_url = f'http://api.scraperapi.com?api_key={api_key}&url={url}&render=true'\n",
        "    response = requests.get(api_url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def extract_reviews(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    review_blocks = soup.select('.review')\n",
        "    reviews = []\n",
        "\n",
        "    for block in review_blocks:\n",
        "        try:\n",
        "            rating = block.select_one('.review-rating').text\n",
        "            title = \"Apple iPhone 13(128GB,Midnight) \"\n",
        "            content = block.select_one('.review-text-content').text.strip()\n",
        "            date = block.select_one('.review-date').text\n",
        "            reviews.append({\n",
        "                'Rating': rating,\n",
        "                'Title': title,\n",
        "                'Content': content,\n",
        "                'Date': date\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting review: {e}\")\n",
        "\n",
        "    return reviews\n",
        "\n",
        "def scrape_and_save_reviews(amazon_product_url, api_key, max_reviews=1000, output_file='amazon_reviews_1000.csv'):\n",
        "    all_reviews = []\n",
        "    page = 1\n",
        "    base_url = f'{amazon_product_url}/ref=cm_cr_arp_d_paging_btm_next_'\n",
        "\n",
        "    while len(all_reviews) < max_reviews:\n",
        "        print(f\"Scraping page {page}...\")\n",
        "        page_url = f\"{base_url}?pageNumber={page}\"\n",
        "\n",
        "        page_content = scrape_amazon_reviews(page_url, api_key)\n",
        "\n",
        "        if page_content:\n",
        "            reviews = extract_reviews(page_content)\n",
        "            all_reviews.extend(reviews)\n",
        "            print(f\"Collected {len(reviews)} reviews from page {page}. Total: {len(all_reviews)}\")\n",
        "            if len(reviews) == 0:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(2)\n",
        "\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Rating', 'Title', 'Content', 'Date']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_reviews)\n",
        "\n",
        "    print(f\"Saved {len(all_reviews)} reviews to {output_file}\")\n",
        "\n",
        "scrape_and_save_reviews(amazon_product_url, My_Scrapper_Api_Key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2ZmaIkejaW_",
        "outputId": "96beba8c-0ae6-4ec6-fe11-2a4f95fb26fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Collected 11 reviews from page 1. Total: 11\n",
            "Scraping page 2...\n",
            "Collected 11 reviews from page 2. Total: 22\n",
            "Scraping page 3...\n",
            "Collected 11 reviews from page 3. Total: 33\n",
            "Scraping page 4...\n",
            "Collected 11 reviews from page 4. Total: 44\n",
            "Scraping page 5...\n",
            "Collected 11 reviews from page 5. Total: 55\n",
            "Scraping page 6...\n",
            "Collected 11 reviews from page 6. Total: 66\n",
            "Scraping page 7...\n",
            "Collected 11 reviews from page 7. Total: 77\n",
            "Scraping page 8...\n",
            "Collected 11 reviews from page 8. Total: 88\n",
            "Scraping page 9...\n",
            "Collected 11 reviews from page 9. Total: 99\n",
            "Scraping page 10...\n",
            "Collected 11 reviews from page 10. Total: 110\n",
            "Scraping page 11...\n",
            "Collected 11 reviews from page 11. Total: 121\n",
            "Scraping page 12...\n",
            "Collected 11 reviews from page 12. Total: 132\n",
            "Scraping page 13...\n",
            "Collected 11 reviews from page 13. Total: 143\n",
            "Scraping page 14...\n",
            "Collected 11 reviews from page 14. Total: 154\n",
            "Scraping page 15...\n",
            "Collected 11 reviews from page 15. Total: 165\n",
            "Scraping page 16...\n",
            "Collected 11 reviews from page 16. Total: 176\n",
            "Scraping page 17...\n",
            "Collected 11 reviews from page 17. Total: 187\n",
            "Scraping page 18...\n",
            "Collected 11 reviews from page 18. Total: 198\n",
            "Scraping page 19...\n",
            "Collected 11 reviews from page 19. Total: 209\n",
            "Scraping page 20...\n",
            "Collected 11 reviews from page 20. Total: 220\n",
            "Scraping page 21...\n",
            "Collected 11 reviews from page 21. Total: 231\n",
            "Scraping page 22...\n",
            "Collected 11 reviews from page 22. Total: 242\n",
            "Scraping page 23...\n",
            "Collected 11 reviews from page 23. Total: 253\n",
            "Scraping page 24...\n",
            "Collected 11 reviews from page 24. Total: 264\n",
            "Scraping page 25...\n",
            "Collected 11 reviews from page 25. Total: 275\n",
            "Scraping page 26...\n",
            "Collected 11 reviews from page 26. Total: 286\n",
            "Scraping page 27...\n",
            "Collected 11 reviews from page 27. Total: 297\n",
            "Scraping page 28...\n",
            "Collected 11 reviews from page 28. Total: 308\n",
            "Scraping page 29...\n",
            "Collected 11 reviews from page 29. Total: 319\n",
            "Scraping page 30...\n",
            "Collected 11 reviews from page 30. Total: 330\n",
            "Scraping page 31...\n",
            "Collected 11 reviews from page 31. Total: 341\n",
            "Scraping page 32...\n",
            "Collected 11 reviews from page 32. Total: 352\n",
            "Scraping page 33...\n",
            "Collected 11 reviews from page 33. Total: 363\n",
            "Scraping page 34...\n",
            "Collected 11 reviews from page 34. Total: 374\n",
            "Scraping page 35...\n",
            "Collected 11 reviews from page 35. Total: 385\n",
            "Scraping page 36...\n",
            "Collected 11 reviews from page 36. Total: 396\n",
            "Scraping page 37...\n",
            "Collected 11 reviews from page 37. Total: 407\n",
            "Scraping page 38...\n",
            "Collected 11 reviews from page 38. Total: 418\n",
            "Scraping page 39...\n",
            "Collected 11 reviews from page 39. Total: 429\n",
            "Scraping page 40...\n",
            "Collected 11 reviews from page 40. Total: 440\n",
            "Scraping page 41...\n",
            "Collected 11 reviews from page 41. Total: 451\n",
            "Scraping page 42...\n",
            "Collected 11 reviews from page 42. Total: 462\n",
            "Scraping page 43...\n",
            "Collected 11 reviews from page 43. Total: 473\n",
            "Scraping page 44...\n",
            "Collected 11 reviews from page 44. Total: 484\n",
            "Scraping page 45...\n",
            "Collected 11 reviews from page 45. Total: 495\n",
            "Scraping page 46...\n",
            "Collected 11 reviews from page 46. Total: 506\n",
            "Scraping page 47...\n",
            "Collected 11 reviews from page 47. Total: 517\n",
            "Scraping page 48...\n",
            "Collected 11 reviews from page 48. Total: 528\n",
            "Scraping page 49...\n",
            "Collected 11 reviews from page 49. Total: 539\n",
            "Scraping page 50...\n",
            "Collected 11 reviews from page 50. Total: 550\n",
            "Scraping page 51...\n",
            "Collected 11 reviews from page 51. Total: 561\n",
            "Scraping page 52...\n",
            "Collected 11 reviews from page 52. Total: 572\n",
            "Scraping page 53...\n",
            "Collected 11 reviews from page 53. Total: 583\n",
            "Scraping page 54...\n",
            "Collected 11 reviews from page 54. Total: 594\n",
            "Scraping page 55...\n",
            "Collected 11 reviews from page 55. Total: 605\n",
            "Scraping page 56...\n",
            "Collected 11 reviews from page 56. Total: 616\n",
            "Scraping page 57...\n",
            "Collected 11 reviews from page 57. Total: 627\n",
            "Scraping page 58...\n",
            "Collected 11 reviews from page 58. Total: 638\n",
            "Scraping page 59...\n",
            "Collected 11 reviews from page 59. Total: 649\n",
            "Scraping page 60...\n",
            "Collected 11 reviews from page 60. Total: 660\n",
            "Scraping page 61...\n",
            "Collected 11 reviews from page 61. Total: 671\n",
            "Scraping page 62...\n",
            "Collected 11 reviews from page 62. Total: 682\n",
            "Scraping page 63...\n",
            "Collected 11 reviews from page 63. Total: 693\n",
            "Scraping page 64...\n",
            "Collected 11 reviews from page 64. Total: 704\n",
            "Scraping page 65...\n",
            "Collected 11 reviews from page 65. Total: 715\n",
            "Scraping page 66...\n",
            "Collected 11 reviews from page 66. Total: 726\n",
            "Scraping page 67...\n",
            "Collected 11 reviews from page 67. Total: 737\n",
            "Scraping page 68...\n",
            "Collected 11 reviews from page 68. Total: 748\n",
            "Scraping page 69...\n",
            "Collected 11 reviews from page 69. Total: 759\n",
            "Scraping page 70...\n",
            "Collected 11 reviews from page 70. Total: 770\n",
            "Scraping page 71...\n",
            "Collected 11 reviews from page 71. Total: 781\n",
            "Scraping page 72...\n",
            "Collected 11 reviews from page 72. Total: 792\n",
            "Scraping page 73...\n",
            "Collected 11 reviews from page 73. Total: 803\n",
            "Scraping page 74...\n",
            "Collected 11 reviews from page 74. Total: 814\n",
            "Scraping page 75...\n",
            "Collected 11 reviews from page 75. Total: 825\n",
            "Scraping page 76...\n",
            "Collected 11 reviews from page 76. Total: 836\n",
            "Scraping page 77...\n",
            "Collected 11 reviews from page 77. Total: 847\n",
            "Scraping page 78...\n",
            "Collected 11 reviews from page 78. Total: 858\n",
            "Scraping page 79...\n",
            "Collected 11 reviews from page 79. Total: 869\n",
            "Scraping page 80...\n",
            "Collected 11 reviews from page 80. Total: 880\n",
            "Scraping page 81...\n",
            "Collected 11 reviews from page 81. Total: 891\n",
            "Scraping page 82...\n",
            "Collected 11 reviews from page 82. Total: 902\n",
            "Scraping page 83...\n",
            "Collected 11 reviews from page 83. Total: 913\n",
            "Scraping page 84...\n",
            "Collected 11 reviews from page 84. Total: 924\n",
            "Scraping page 85...\n",
            "Collected 11 reviews from page 85. Total: 935\n",
            "Scraping page 86...\n",
            "Collected 11 reviews from page 86. Total: 946\n",
            "Scraping page 87...\n",
            "Collected 11 reviews from page 87. Total: 957\n",
            "Scraping page 88...\n",
            "Collected 11 reviews from page 88. Total: 968\n",
            "Scraping page 89...\n",
            "Collected 11 reviews from page 89. Total: 979\n",
            "Scraping page 90...\n",
            "Collected 11 reviews from page 90. Total: 990\n",
            "Scraping page 91...\n",
            "Collected 11 reviews from page 91. Total: 1001\n",
            "Saved 1001 reviews to amazon_reviews_1000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tnRfyCSitf-",
        "outputId": "fcc9346c-d12a-4349-8c59-ad6372fe5829"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7571b99f-a6e8-48f4-f983-75166100ea84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Content  \\\n",
              "0  This review is based on the product quality an...   \n",
              "1  iPhone 13, midnight, 256gb, unlocked version, ...   \n",
              "2  The phone is in a very good condition:No damag...   \n",
              "3  This was a great buy for me and the phone work...   \n",
              "4  Ordered the iPhone 13 in Midnight for T-Mobile...   \n",
              "\n",
              "                                     Cleaned_Content  \n",
              "0  review base product qualiti condit phone came ...  \n",
              "1  iphon midnight gb unlock version seller direct...  \n",
              "2  phone good conditionno damag screen clean fine...  \n",
              "3  great buy phone work extrem well far refurbish...  \n",
              "4  order iphon midnight tmobil phone came near pe...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3d5298b-4f84-4534-9bfc-703e8d2759a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Cleaned_Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This review is based on the product quality an...</td>\n",
              "      <td>review base product qualiti condit phone came ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>iPhone 13, midnight, 256gb, unlocked version, ...</td>\n",
              "      <td>iphon midnight gb unlock version seller direct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The phone is in a very good condition:No damag...</td>\n",
              "      <td>phone good conditionno damag screen clean fine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This was a great buy for me and the phone work...</td>\n",
              "      <td>great buy phone work extrem well far refurbish...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ordered the iPhone 13 in Midnight for T-Mobile...</td>\n",
              "      <td>order iphon midnight tmobil phone came near pe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3d5298b-4f84-4534-9bfc-703e8d2759a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d3d5298b-4f84-4534-9bfc-703e8d2759a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d3d5298b-4f84-4534-9bfc-703e8d2759a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-22839b1e-5189-41cc-9598-d064d7749d9a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22839b1e-5189-41cc-9598-d064d7749d9a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-22839b1e-5189-41cc-9598-d064d7749d9a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data[['Content', 'Cleaned_Content']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"iPhone 13, midnight, 256gb, unlocked version, seller = DirectCertified.  I ordered 2 for my kids.  Both phones were practically same condition.Cosmetic - One phone had a very tiny stress fracture on the screen that measured approx 1\\\" across.  It was only noticeable at direct light and 6\\\" away.  After laying a screen protector, it is not noticeable.  Both iphones had 1-2 tiny knicks on the screen.  Again, both are only noticeable up front and after laying a screen protector, it is barely noticeable.  The casings on both were near flawless.  I can tell the previous owners used screen protectors and cases with longevity in mind.  I grade both phones a 95% on appearance and like new cosmetically.Battery - One phone had 85% and other had 86%.  A bit on the lower end but its all a gamble going the refurbished route.  Not bad enough for me to deal with doing an exchange and take a chance I may get a worse condition phone.  I suspect its the original batteries in these 2 units.Function - speaker is loud, face ID works, touchscreen is good, sidebar buttons all work, rear and front cameras are clear, everything works as it should.  Doesn't seem to have aftermarket parts used on these phones as there is no pop up notification indicating so.  I suspect everything on these 2 phones were original.I will update this review after few weeks of use.  I am more curious on battery life so we'll see...8/18/2024 update:  kiddos reported the phone battery holds its charge fine. 1 full charge lasts them the whole day. They're using a fast charger purchased off Amazon and so far so good!\",\n          \"Ordered the iPhone 13 in Midnight for T-Mobile. the phone came in near perfect condition with minor scratches on the camera area (doesn\\u2019t affect camera / picture quality at all). the phone came with a preinstalled glass screen protector. i was especially surprised and pleased by the 96% battery health!! definitely a gamble when it comes to buying refurbished / renewed phones but it ended up working out. the second hand seller i purchased through was Synergy, paid $377 (including tax). little more than half the price than a new phone is totally worth it. also did a fair amount of research and apparently the iphone 14 and 13 are the same exact product, so would recommend buying the 13 to save some money!\",\n          \"The phone is in a very good condition:No damages, the screen is clean and fine.The battery is 84% which is not good.The phone performs and functions normally, the phone looks good but their is some scratches around the frame of the camera (not on the lens of the camera) doesn't affect the camera. Overall a good deal.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned_Content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"iphon midnight gb unlock version seller directcertifi order kid phone practic conditioncosmet one phone tini stress fractur screen measur approx across notic direct light away lay screen protector notic iphon tini knick screen notic front lay screen protector bare notic case near flawless tell previou owner use screen protector case longev mind grade phone appear like new cosmeticallybatteri one phone bit lower end gambl go refurbish rout bad enough deal exchang take chanc may get wors condit phone suspect origin batteri unitsfunct speaker loud face id work touchscreen good sidebar button work rear front camera clear everyth work doesnt seem aftermarket part use phone pop notif indic suspect everyth phone originali updat review week use curiou batteri life well see updat kiddo report phone batteri hold charg fine full charg last whole day theyr use fast charger purchas amazon far good\",\n          \"order iphon midnight tmobil phone came near perfect condit minor scratch camera area doesnt affect camera pictur qualiti phone came preinstal glass screen protector especi surpris plea batteri health definit gambl come buy refurbish renew phone end work second hand seller purchas synergi paid includ tax littl half price new phone total worth also fair amount research appar iphon exact product would recommend buy save money\",\n          \"phone good conditionno damag screen clean fineth batteri goodth phone perform function normal phone look good scratch around frame camera len camera doesnt affect camera overal good deal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "data = pd.read_csv(\"/content/amazon_reviews_1000.csv\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1. Remove noise\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 2. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 3. Remove stopwords\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # 4. Lowercase all texts\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # 5. Stemming\n",
        "    stemmed_words = [ps.stem(word) for word in words]\n",
        "\n",
        "    # 6. Lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "data['Cleaned_Content'] = data['Content'].apply(clean_text)\n",
        "data.to_csv('cleaned_amazon_reviews_1000.csv', index=False)\n",
        "data[['Content', 'Cleaned_Content']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnUsuXBsPZwM",
        "outputId": "bc78fbcf-b83c-410e-d2fe-0292099f7aff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ad257a-c390-43ed-efa4-adb40474e283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech (POS) Counts:\n",
            "Nouns: 22\n",
            "Verbs: 9\n",
            "Adjectives: 6\n",
            "Adverbs: 3\n",
            "\n",
            "Constituency Parsing (Dependency Roles):\n",
            "['compound', 'compound', 'compound', 'compound', 'compound', 'nsubj', 'ROOT', 'amod', 'amod', 'compound', 'npadvmod', 'advcl', 'compound', 'compound', 'compound', 'compound', 'nsubj', 'ccomp', 'compound', 'nsubj', 'ccomp', 'ccomp', 'neg', 'compound', 'compound', 'attr', 'nsubj', 'nsubj', 'ROOT', 'nsubj', 'advmod', 'ccomp', 'xcomp', 'compound', 'advmod', 'aux', 'neg', 'compound', 'compound', 'compound', 'nmod', 'compound', 'compound', 'compound', 'compound', 'compound', 'aux', 'neg', 'amod', 'nsubj', 'ccomp', 'amod', 'nsubj', 'ccomp', 'compound', 'dobj', 'advmod']\n",
            "\n",
            "Dependency Parsing (Word -> Head Relation):\n",
            "Word: review, Dependency: compound, Head: product\n",
            "Word: base, Dependency: compound, Head: product\n",
            "Word: product, Dependency: compound, Head: phone\n",
            "Word: qualiti, Dependency: compound, Head: phone\n",
            "Word: condit, Dependency: compound, Head: phone\n",
            "Word: phone, Dependency: nsubj, Head: came\n",
            "Word: came, Dependency: ROOT, Head: came\n",
            "Word: nice, Dependency: amod, Head: visibl\n",
            "Word: neat, Dependency: amod, Head: visibl\n",
            "Word: scratch, Dependency: compound, Head: visibl\n",
            "Word: visibl, Dependency: npadvmod, Head: came\n",
            "Word: ware, Dependency: advcl, Head: came\n",
            "Word: tear, Dependency: compound, Head: easi\n",
            "Word: easi, Dependency: compound, Head: detail\n",
            "Word: program, Dependency: compound, Head: unlock\n",
            "Word: unlock, Dependency: compound, Head: detail\n",
            "Word: detail, Dependency: nsubj, Head: saw\n",
            "Word: saw, Dependency: ccomp, Head: ware\n",
            "Word: batteri, Dependency: compound, Head: life\n",
            "Word: life, Dependency: nsubj, Head: mean\n",
            "Word: mean, Dependency: ccomp, Head: saw\n",
            "Word: was, Dependency: ccomp, Head: saw\n",
            "Word: nt, Dependency: neg, Head: was\n",
            "Word: replac, Dependency: compound, Head: fresher\n",
            "Word: fresher, Dependency: compound, Head: one\n",
            "Word: one, Dependency: attr, Head: was\n",
            "Word: i, Dependency: nsubj, Head: m\n",
            "Word: m, Dependency: nsubj, Head: expect\n",
            "Word: expect, Dependency: ROOT, Head: expect\n",
            "Word: chang, Dependency: nsubj, Head: function\n",
            "Word: sooner, Dependency: advmod, Head: function\n",
            "Word: function, Dependency: ccomp, Head: expect\n",
            "Word: seem, Dependency: xcomp, Head: function\n",
            "Word: work, Dependency: compound, Head: fine\n",
            "Word: fine, Dependency: advmod, Head: seem\n",
            "Word: have, Dependency: aux, Head: think\n",
            "Word: nt, Dependency: neg, Head: have\n",
            "Word: issu, Dependency: compound, Head: sinc\n",
            "Word: appl, Dependency: compound, Head: sinc\n",
            "Word: sinc, Dependency: compound, Head: phone\n",
            "Word: appl, Dependency: nmod, Head: fame\n",
            "Word: notori, Dependency: compound, Head: fame\n",
            "Word: fame, Dependency: compound, Head: disabl\n",
            "Word: disabl, Dependency: compound, Head: phone\n",
            "Word: featur, Dependency: compound, Head: phone\n",
            "Word: phone, Dependency: compound, Head: part\n",
            "Word: do, Dependency: aux, Head: part\n",
            "Word: nt, Dependency: neg, Head: part\n",
            "Word: genuin, Dependency: amod, Head: part\n",
            "Word: part, Dependency: nsubj, Head: think\n",
            "Word: think, Dependency: ccomp, Head: expect\n",
            "Word: good, Dependency: amod, Head: vender\n",
            "Word: vender, Dependency: nsubj, Head: buy\n",
            "Word: buy, Dependency: ccomp, Head: think\n",
            "Word: phone, Dependency: compound, Head: budget\n",
            "Word: budget, Dependency: dobj, Head: buy\n",
            "Word: tight, Dependency: advmod, Head: buy\n",
            "\n",
            "Named Entity Recognition (Entities):\n",
            "Entity: chang, Type: PERSON\n",
            "\n",
            "Total POS Counts Across All Reviews:\n",
            "Total Nouns: 20601\n",
            "Total Verbs: 7383\n",
            "Total Adjectives: 7377\n",
            "Total Adverbs: 2241\n",
            "\n",
            "Entity Counts Across All Reviews:\n",
            "PERSON: 1644\n",
            "TIME: 182\n",
            "CARDINAL: 728\n",
            "GPE: 313\n",
            "DATE: 182\n",
            "ORDINAL: 182\n",
            "ORG: 1041\n",
            "FAC: 74\n",
            "PRODUCT: 91\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "data1 = pd.read_csv('cleaned_amazon_reviews_1000.csv')\n",
        "\n",
        "def analyze_text(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # 1. POS Tagging\n",
        "    pos_counts = Counter([token.pos_ for token in doc])\n",
        "    noun_count = pos_counts.get(\"NOUN\", 0)\n",
        "    verb_count = pos_counts.get(\"VERB\", 0)\n",
        "    adj_count = pos_counts.get(\"ADJ\", 0)\n",
        "    adv_count = pos_counts.get(\"ADV\", 0)\n",
        "\n",
        "    # 2. Constituency Parsing and Dependency Parsing\n",
        "    constituency_parse = [token.dep_ for token in doc]\n",
        "    dependency_parse = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "    # 3. Named Entity Recognition\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    return {\n",
        "        \"noun_count\": noun_count,\n",
        "        \"verb_count\": verb_count,\n",
        "        \"adj_count\": adj_count,\n",
        "        \"adv_count\": adv_count,\n",
        "        \"constituency_parse\": constituency_parse,\n",
        "        \"dependency_parse\": dependency_parse,\n",
        "        \"entities\": entities\n",
        "    }\n",
        "\n",
        "sample_review = data1['Cleaned_Content'].iloc[0]\n",
        "analysis_result = analyze_text(sample_review)\n",
        "\n",
        "print(\"Parts of Speech (POS) Counts:\")\n",
        "print(f\"Nouns: {analysis_result['noun_count']}\")\n",
        "print(f\"Verbs: {analysis_result['verb_count']}\")\n",
        "print(f\"Adjectives: {analysis_result['adj_count']}\")\n",
        "print(f\"Adverbs: {analysis_result['adv_count']}\")\n",
        "\n",
        "print(\"\\nConstituency Parsing (Dependency Roles):\")\n",
        "print(analysis_result['constituency_parse'])\n",
        "\n",
        "print(\"\\nDependency Parsing (Word -> Head Relation):\")\n",
        "for dep in analysis_result['dependency_parse']:\n",
        "    print(f\"Word: {dep[0]}, Dependency: {dep[1]}, Head: {dep[2]}\")\n",
        "\n",
        "print(\"\\nNamed Entity Recognition (Entities):\")\n",
        "for entity in analysis_result['entities']:\n",
        "    print(f\"Entity: {entity[0]}, Type: {entity[1]}\")\n",
        "\n",
        "def analyze_all_reviews(df):\n",
        "    total_nouns = 0\n",
        "    total_verbs = 0\n",
        "    total_adj = 0\n",
        "    total_adv = 0\n",
        "    all_entities = Counter()\n",
        "\n",
        "    for text in df['Cleaned_Content']:\n",
        "        result = analyze_text(text)\n",
        "        total_nouns += result['noun_count']\n",
        "        total_verbs += result['verb_count']\n",
        "        total_adj += result['adj_count']\n",
        "        total_adv += result['adv_count']\n",
        "        all_entities.update([ent[1] for ent in result['entities']])\n",
        "\n",
        "    return {\n",
        "        \"total_nouns\": total_nouns,\n",
        "        \"total_verbs\": total_verbs,\n",
        "        \"total_adjectives\": total_adj,\n",
        "        \"total_adverbs\": total_adv,\n",
        "        \"entity_counts\": dict(all_entities)\n",
        "    }\n",
        "\n",
        "overall_results = analyze_all_reviews(data1)\n",
        "\n",
        "print(\"\\nTotal POS Counts Across All Reviews:\")\n",
        "print(f\"Total Nouns: {overall_results['total_nouns']}\")\n",
        "print(f\"Total Verbs: {overall_results['total_verbs']}\")\n",
        "print(f\"Total Adjectives: {overall_results['total_adjectives']}\")\n",
        "print(f\"Total Adverbs: {overall_results['total_adverbs']}\")\n",
        "\n",
        "print(\"\\nEntity Counts Across All Reviews:\")\n",
        "for entity_type, count in overall_results['entity_counts'].items():\n",
        "    print(f\"{entity_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "# \"The face the 1st question quiet challenging, but with the help of the chatgpt and course materials helped me a bit.\n",
        "# I was in a dilemma to which choose question but finally chossed 1st part of the question 1 and used scrapper api key to get the reviews from\n",
        "# amazon. The most interesting part was cleanning the amazon reviews that we got from the question one.\n",
        "# Based on the assignment difficulty, the time was too short to complete it but with extended deadline t made the things easier.\""
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}